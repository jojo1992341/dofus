{"cells":[{"cell_type":"markdown","metadata":{"id":"a_3m83lHhI8y"},"source":["# Convertisseur et Traducteur de fichiers ePub\n","\n","Ce notebook permet de :\n","1. Convertir des fichiers ePub en DOCX\n","2. Traduire les fichiers DOCX en français\n","3. Reconvertir les fichiers traduits en ePub"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41292,"status":"ok","timestamp":1735125013761,"user":{"displayName":"Jérôme Gamelin","userId":"10987259960980202661"},"user_tz":-60},"id":"vfrU5PuyChLz","outputId":"92598cf6-81bc-4df8-fba3-fc674eaac681"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9364,"status":"ok","timestamp":1735125025753,"user":{"displayName":"Jérôme Gamelin","userId":"10987259960980202661"},"user_tz":-60},"id":"8V-Lwn8MhI8-","outputId":"cc8d7e24-78bf-4dae-f331-7474d15c6ebd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting googletrans==3.1.0a0\n","  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n","  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3-\u003egoogletrans==3.1.0a0) (2024.12.14)\n","Collecting hstspreload (from httpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3-\u003egoogletrans==3.1.0a0) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n","Collecting idna==2.* (from httpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting rfc3986\u003c2,\u003e=1.3 (from httpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting h11\u003c0.10,\u003e=0.8 (from httpcore==0.9.*-\u003ehttpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n","Collecting h2==3.* (from httpcore==0.9.*-\u003ehttpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting hyperframe\u003c6,\u003e=5.2.0 (from h2==3.*-\u003ehttpcore==0.9.*-\u003ehttpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting hpack\u003c4,\u003e=3.0 (from h2==3.*-\u003ehttpcore==0.9.*-\u003ehttpx==0.13.3-\u003egoogletrans==3.1.0a0)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n","Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=ea900f25b2978113602c6b807dbd9056fc21206aad2efc1c81fa833bcde0fac9\n","  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: h11\n","    Found existing installation: h11 0.14.0\n","    Uninstalling h11-0.14.0:\n","      Successfully uninstalled h11-0.14.0\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","  Attempting uninstall: httpcore\n","    Found existing installation: httpcore 1.0.7\n","    Uninstalling httpcore-1.0.7:\n","      Successfully uninstalled httpcore-1.0.7\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.28.1\n","    Uninstalling httpx-0.28.1:\n","      Successfully uninstalled httpx-0.28.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langsmith 0.2.3 requires httpx\u003c1,\u003e=0.23.0, but you have httpx 0.13.3 which is incompatible.\n","openai 1.57.4 requires httpx\u003c1,\u003e=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"]}],"source":["#!pip install ebooklib beautifulsoup4 googletrans==3.1.0a0 html2text psutil deep-translator\u003e=1.11.4\n","#!pip install -r /content/requirements.txt\n","!pip install googletrans==3.1.0a0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Om4GysCJj6I-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Le fichier Rogue_Knight_by_Illuviar-XhqrMJNx.epub a déjà été traduit. Passage au suivant...\n","Le fichier Harry_Potter_and_the_Second_Chance_by_PropheticScript-kz9l63yg.epub a déjà été traduit. Passage au suivant...\n","Le fichier Meant_to_be_by_vedakshu2006-bfxypana.epub a déjà été traduit. Passage au suivant...\n","Le fichier Enchanting_Melodies_by_athassprkr-azjeosdz.epub a déjà été traduit. Passage au suivant...\n","Le fichier Defrost_by_1Valor1-3o5a79mz.epub a déjà été traduit. Passage au suivant...\n","Le fichier HARRY_POTTER_THE_LOST_LEGACY_by_peverell_magic-ruvkkh9s.epub a déjà été traduit. Passage au suivant...\n","Le fichier The_Chosen_Six_by_Tribun-kBnFE6Zd.epub a déjà été traduit. Passage au suivant...\n","Le fichier Harry_Potter_and_Sirius_Legacy_by_C_A_Rotwang-p6xrbvef.epub a déjà été traduit. Passage au suivant...\n","Le fichier A_Promise_Given_by_Umthieral-isd4thls.epub a déjà été traduit. Passage au suivant...\n","Le fichier Cavorting_with_Death_by_Rokakku-fsidv6jf.epub a déjà été traduit. Passage au suivant...\n","Le fichier Sling_Shots_by_illjwamh-14BCQue2.epub a déjà été traduit. Passage au suivant...\n","Traitement de Part_Lion_Part_Snake_by_MrBamforth-e6effl8t.epub...\n","Fichier extrait dans /content/drive/MyDrive/fanfictions/temp/extraction\n","Traduction de introduction.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 9\n","Traduction de nav.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 1\n","Traduction de chap_1.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 166\n","Traduction de chap_2.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 176\n","Traduction de chap_3.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 93\n","Traduction de chap_4.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 164\n","Traduction de chap_5.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 151\n","Traduction de chap_6.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 214\n","Traduction de chap_7.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 218\n","Traduction de chap_8.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 193\n","Traduction de chap_9.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 314\n","Traduction de chap_10.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 143\n","Traduction de chap_11.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 241\n","Traduction de chap_12.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 156\n","Traduction de chap_13.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 222\n","Traduction de chap_14.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 243\n","Traduction de chap_15.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 204\n","Traduction de chap_16.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 168\n","Traduction de chap_17.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 438\n","Traduction de chap_18.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 173\n","Traduction de chap_19.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 182\n","Traduction de chap_20.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 200\n","Traduction de chap_21.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 221\n","Traduction de chap_22.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 233\n","Traduction de chap_23.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 244\n","Traduction de chap_24.xhtml\n","Éléments traduits: 0, Éléments trouvés dans le cache: 147\n","Traduction de chap_25.xhtml\n","Éléments traduits: 56, Éléments trouvés dans le cache: 58\n","Traduction de chap_26.xhtml\n","Éléments traduits: 141, Éléments trouvés dans le cache: 0\n","Traduction de chap_27.xhtml\n","Éléments traduits: 146, Éléments trouvés dans le cache: 3\n","Traduction de chap_28.xhtml\n","Éléments traduits: 168, Éléments trouvés dans le cache: 0\n","Traduction de chap_29.xhtml\n","Éléments traduits: 241, Éléments trouvés dans le cache: 6\n","Traduction de chap_30.xhtml\n","Éléments traduits: 239, Éléments trouvés dans le cache: 4\n","Traduction de chap_31.xhtml\n","Éléments traduits: 109, Éléments trouvés dans le cache: 0\n","Traduction de chap_32.xhtml\n","Éléments traduits: 226, Éléments trouvés dans le cache: 6\n","Traduction de chap_33.xhtml\n","Éléments traduits: 136, Éléments trouvés dans le cache: 0\n","Traduction de chap_34.xhtml\n","Éléments traduits: 142, Éléments trouvés dans le cache: 3\n","Traduction de chap_35.xhtml\n","Éléments traduits: 134, Éléments trouvés dans le cache: 1\n","Traduction de chap_36.xhtml\n","Éléments traduits: 136, Éléments trouvés dans le cache: 1\n","Traduction de chap_37.xhtml\n","Éléments traduits: 140, Éléments trouvés dans le cache: 2\n","Traduction de chap_38.xhtml\n","Éléments traduits: 230, Éléments trouvés dans le cache: 3\n","Création du fichier ePub: /content/drive/MyDrive/fanfictions/vf/Part_Lion_Part_Snake_by_MrBamforth-e6effl8t_fr.epub\n","Terminé! Fichier traduit sauvegardé: /content/drive/MyDrive/fanfictions/vf/Part_Lion_Part_Snake_by_MrBamforth-e6effl8t_fr.epub\n","Dossiers temporaires nettoyés\n"]}],"source":["import os\n","import glob\n","import shutil\n","import zipfile\n","from bs4 import BeautifulSoup\n","from googletrans import Translator\n","import time\n","import json\n","import hashlib\n","\n","def ensure_directories():\n","    \"\"\"Crée les dossiers nécessaires\"\"\"\n","    directories = [\n","        '/content/drive/MyDrive/fanfictions/vo',\n","        '/content/drive/MyDrive/fanfictions/temp/extraction',\n","        '/content/drive/MyDrive/fanfictions/temp/vf',\n","        '/content/drive/MyDrive/fanfictions/vf',\n","        '/content/drive/MyDrive/fanfictions/logs'\n","    ]\n","    for directory in directories:\n","        os.makedirs(directory, exist_ok=True)\n","\n","class TranslationCache:\n","    def __init__(self):\n","        self.cache_file = '/content/drive/MyDrive/fanfictions/logs/translation_cache.json'\n","        self.cache = self._load_cache()\n","\n","    def _load_cache(self):\n","        \"\"\"Charge le cache depuis le fichier\"\"\"\n","        if os.path.exists(self.cache_file):\n","            try:\n","                with open(self.cache_file, 'r', encoding='utf-8') as f:\n","                    return json.load(f)\n","            except Exception as e:\n","                print(f\"Erreur lors du chargement du cache: {e}\")\n","                return {}\n","        return {}\n","\n","    def _save_cache(self):\n","        \"\"\"Sauvegarde le cache dans le fichier\"\"\"\n","        try:\n","            with open(self.cache_file, 'w', encoding='utf-8') as f:\n","                json.dump(self.cache, f, ensure_ascii=False, indent=2)\n","        except Exception as e:\n","            print(f\"Erreur lors de la sauvegarde du cache: {e}\")\n","\n","    def get_translation(self, text):\n","        \"\"\"Récupère une traduction du cache\"\"\"\n","        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()\n","        return self.cache.get(text_hash)\n","\n","    def add_translation(self, text, translation):\n","        \"\"\"Ajoute une traduction au cache\"\"\"\n","        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()\n","        self.cache[text_hash] = translation\n","        self._save_cache()\n","\n","def extract_epub(epub_path, extract_dir):\n","    \"\"\"Extrait le contenu du fichier ePub\"\"\"\n","    with zipfile.ZipFile(epub_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_dir)\n","    print(f\"Fichier extrait dans {extract_dir}\")\n","\n","def find_xhtml_files(directory):\n","    \"\"\"Trouve tous les fichiers XHTML dans le dossier et ses sous-dossiers\"\"\"\n","    xhtml_files = []\n","    for root, _, files in os.walk(directory):\n","        for file in files:\n","            if file.endswith(('.xhtml', '.html', '.htm')):\n","                xhtml_files.append(os.path.join(root, file))\n","    return xhtml_files\n","\n","def translate_xhtml_file(input_path, output_path, translation_cache):\n","    \"\"\"Traduit un fichier XHTML en utilisant le cache\"\"\"\n","    print(f\"Traduction de {os.path.basename(input_path)}\")\n","    translator = Translator()\n","\n","    # Lire le fichier source\n","    with open(input_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","\n","    # Parser le contenu HTML\n","    soup = BeautifulSoup(content, 'html.parser')\n","\n","    # Sauvegarder les styles\n","    style_tags = soup.find_all('style')\n","    original_styles = ''.join(str(style) for style in style_tags)\n","\n","    # Traduire tous les éléments de texte\n","    elements_translated = 0\n","    elements_cached = 0\n","\n","    for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'span']):\n","        if element.string and element.string.strip():\n","            original_text = element.string.strip()\n","\n","            # Vérifier dans le cache\n","            cached_translation = translation_cache.get_translation(original_text)\n","            if cached_translation:\n","                element.string = cached_translation\n","                elements_cached += 1\n","                continue\n","\n","            try:\n","                # Traduire le texte\n","                translated = translator.translate(original_text, dest='fr')\n","                element.string = translated.text\n","                # Ajouter au cache\n","                translation_cache.add_translation(original_text, translated.text)\n","                elements_translated += 1\n","                # Petit délai pour éviter les limitations de l'API\n","                time.sleep(0.5)\n","            except Exception as e:\n","                print(f\"Erreur de traduction: {e}\")\n","\n","    print(f\"Éléments traduits: {elements_translated}, Éléments trouvés dans le cache: {elements_cached}\")\n","\n","    # Sauvegarder le fichier traduit\n","    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","    with open(output_path, 'w', encoding='utf-8') as file:\n","        file.write(str(soup))\n","\n","def create_epub(input_dir, output_path):\n","    \"\"\"Crée un nouveau fichier ePub à partir des fichiers du dossier\"\"\"\n","    print(f\"Création du fichier ePub: {output_path}\")\n","\n","    # Créer le fichier ZIP (ePub)\n","    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n","        # Ajouter le mimetype sans compression\n","        mimetype_path = os.path.join(input_dir, 'mimetype')\n","        if os.path.exists(mimetype_path):\n","            zf.write(mimetype_path, 'mimetype', compress_type=zipfile.ZIP_STORED)\n","\n","        # Ajouter tous les autres fichiers\n","        for root, _, files in os.walk(input_dir):\n","            for file in files:\n","                if file != 'mimetype':  # Skip mimetype as it's already added\n","                    file_path = os.path.join(root, file)\n","                    arcname = os.path.relpath(file_path, input_dir)\n","                    zf.write(file_path, arcname)\n","\n","def clean_temp_directories():\n","    \"\"\"Nettoie les dossiers temporaires\"\"\"\n","    temp_dirs = ['/content/drive/MyDrive/fanfictions/temp/extraction', '/content/drive/MyDrive/fanfictions/temp/vf']\n","    for dir_path in temp_dirs:\n","        if os.path.exists(dir_path):\n","            shutil.rmtree(dir_path)\n","            os.makedirs(dir_path)\n","    print(\"Dossiers temporaires nettoyés\")\n","\n","def process_files():\n","    \"\"\"Traite tous les fichiers ePub dans le dossier d'entrée\"\"\"\n","    ensure_directories()\n","    translation_cache = TranslationCache()\n","    epub_files = glob.glob('/content/drive/MyDrive/fanfictions/vo/*.epub')\n","\n","    for epub_file in epub_files:\n","        try:\n","            base_name = os.path.basename(epub_file)\n","            name_without_ext = os.path.splitext(base_name)[0]\n","\n","            # Vérifier si le fichier a déjà été traduit\n","            output_epub = f'/content/drive/MyDrive/fanfictions/vf/{name_without_ext}_fr.epub'\n","            if os.path.exists(output_epub):\n","                print(f\"Le fichier {base_name} a déjà été traduit. Passage au suivant...\")\n","                continue\n","\n","            print(f'Traitement de {base_name}...')\n","\n","            # Nettoyer le dossier d'extraction\n","            extraction_dir = '/content/drive/MyDrive/fanfictions/temp/extraction'\n","            if os.path.exists(extraction_dir):\n","                shutil.rmtree(extraction_dir)\n","            os.makedirs(extraction_dir)\n","\n","            # 1. Extraire l'ePub\n","            extract_epub(epub_file, extraction_dir)\n","\n","            # 2. Trouver et traduire les fichiers XHTML\n","            xhtml_files = find_xhtml_files(extraction_dir)\n","            for xhtml_file in xhtml_files:\n","                # Créer le chemin de sortie en préservant la structure des dossiers\n","                rel_path = os.path.relpath(xhtml_file, extraction_dir)\n","                output_path = os.path.join('/content/drive/MyDrive/fanfictions/temp/vf', rel_path)\n","\n","                # Traduire le fichier\n","                translate_xhtml_file(xhtml_file, output_path, translation_cache)\n","\n","                # Remplacer le fichier original par la version traduite\n","                shutil.copy2(output_path, xhtml_file)\n","\n","            # 3. Créer le nouveau fichier ePub\n","            create_epub(extraction_dir, output_epub)\n","\n","            print(f'Terminé! Fichier traduit sauvegardé: {output_epub}')\n","\n","            # 4. Nettoyer les dossiers temporaires\n","            clean_temp_directories()\n","\n","        except Exception as e:\n","            print(f\"Erreur lors du traitement de {base_name}: {str(e)}\")\n","            # Nettoyer même en cas d'erreur\n","            clean_temp_directories()\n","            continue\n","\n","if __name__ == \"__main__\":\n","    process_files()"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}